{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'data/split/train'\n",
    "val_dir = 'data/split/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RGB channel value is in the range [0, 255] . This is not ideal for neural networks; in general you should find a way to reduce your input value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=64,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        val_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=64,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.preprocessing.image.DirectoryIterator object at 0x7fe47db59390>\n"
     ]
    }
   ],
   "source": [
    "print(train_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model = Sequential()\n",
    "\n",
    "emotion_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\n",
    "emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "emotion_model.add(Flatten())\n",
    "emotion_model.add(Dense(1024, activation='relu'))\n",
    "emotion_model.add(Dropout(0.1))\n",
    "emotion_model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tvc/miniconda3/envs/ml/lib/python3.7/site-packages/keras/optimizers/optimizer_v2/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3/449 [..............................] - ETA: 12s - loss: 1.9064 - accuracy: 0.2500 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-26 03:30:41.233276: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_3/dropout_9/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "449/449 [==============================] - 13s 27ms/step - loss: 1.7563 - accuracy: 0.2833 - val_loss: 1.6317 - val_accuracy: 0.3819\n",
      "Epoch 2/50\n",
      "449/449 [==============================] - 11s 25ms/step - loss: 1.5696 - accuracy: 0.3975 - val_loss: 1.4999 - val_accuracy: 0.4313\n",
      "Epoch 3/50\n",
      "449/449 [==============================] - 13s 30ms/step - loss: 1.4808 - accuracy: 0.4355 - val_loss: 1.4351 - val_accuracy: 0.4565\n",
      "Epoch 4/50\n",
      "449/449 [==============================] - 14s 30ms/step - loss: 1.4092 - accuracy: 0.4664 - val_loss: 1.3607 - val_accuracy: 0.4886\n",
      "Epoch 5/50\n",
      "449/449 [==============================] - 14s 31ms/step - loss: 1.3493 - accuracy: 0.4875 - val_loss: 1.3368 - val_accuracy: 0.4979\n",
      "Epoch 6/50\n",
      "449/449 [==============================] - 14s 31ms/step - loss: 1.2951 - accuracy: 0.5108 - val_loss: 1.2837 - val_accuracy: 0.5116\n",
      "Epoch 7/50\n",
      "449/449 [==============================] - 14s 31ms/step - loss: 1.2489 - accuracy: 0.5288 - val_loss: 1.2615 - val_accuracy: 0.5199\n",
      "Epoch 8/50\n",
      "449/449 [==============================] - 14s 31ms/step - loss: 1.2084 - accuracy: 0.5462 - val_loss: 1.2331 - val_accuracy: 0.5330\n",
      "Epoch 9/50\n",
      "449/449 [==============================] - 14s 31ms/step - loss: 1.1720 - accuracy: 0.5585 - val_loss: 1.2253 - val_accuracy: 0.5379\n",
      "Epoch 10/50\n",
      "449/449 [==============================] - 14s 31ms/step - loss: 1.1274 - accuracy: 0.5775 - val_loss: 1.1968 - val_accuracy: 0.5534\n",
      "Epoch 11/50\n",
      "449/449 [==============================] - 14s 31ms/step - loss: 1.0881 - accuracy: 0.5989 - val_loss: 1.1665 - val_accuracy: 0.5613\n",
      "Epoch 12/50\n",
      "449/449 [==============================] - 14s 31ms/step - loss: 1.0559 - accuracy: 0.6091 - val_loss: 1.1559 - val_accuracy: 0.5711\n",
      "Epoch 13/50\n",
      "449/449 [==============================] - 14s 31ms/step - loss: 1.0102 - accuracy: 0.6293 - val_loss: 1.1512 - val_accuracy: 0.5719\n",
      "Epoch 14/50\n",
      "449/449 [==============================] - 14s 32ms/step - loss: 0.9711 - accuracy: 0.6416 - val_loss: 1.1349 - val_accuracy: 0.5759\n",
      "Epoch 15/50\n",
      "449/449 [==============================] - 14s 32ms/step - loss: 0.9365 - accuracy: 0.6550 - val_loss: 1.1359 - val_accuracy: 0.5823\n",
      "Epoch 16/50\n",
      "449/449 [==============================] - 14s 32ms/step - loss: 0.8919 - accuracy: 0.6730 - val_loss: 1.1352 - val_accuracy: 0.5892\n",
      "Epoch 17/50\n",
      "449/449 [==============================] - 12s 28ms/step - loss: 0.8521 - accuracy: 0.6914 - val_loss: 1.1525 - val_accuracy: 0.5840\n",
      "Epoch 18/50\n",
      "449/449 [==============================] - 18s 41ms/step - loss: 0.8144 - accuracy: 0.7056 - val_loss: 1.1354 - val_accuracy: 0.5915\n",
      "Epoch 19/50\n",
      "449/449 [==============================] - 14s 30ms/step - loss: 0.7669 - accuracy: 0.7224 - val_loss: 1.1617 - val_accuracy: 0.5935\n",
      "Epoch 20/50\n",
      "449/449 [==============================] - 12s 28ms/step - loss: 0.7258 - accuracy: 0.7375 - val_loss: 1.1564 - val_accuracy: 0.5985\n",
      "Epoch 21/50\n",
      "449/449 [==============================] - 13s 28ms/step - loss: 0.6863 - accuracy: 0.7555 - val_loss: 1.1496 - val_accuracy: 0.6013\n",
      "Epoch 22/50\n",
      "449/449 [==============================] - 12s 27ms/step - loss: 0.6443 - accuracy: 0.7704 - val_loss: 1.1868 - val_accuracy: 0.5992\n",
      "Epoch 23/50\n",
      "449/449 [==============================] - 12s 27ms/step - loss: 0.6102 - accuracy: 0.7827 - val_loss: 1.1855 - val_accuracy: 0.5986\n",
      "Epoch 24/50\n",
      "449/449 [==============================] - 13s 28ms/step - loss: 0.5593 - accuracy: 0.8034 - val_loss: 1.2079 - val_accuracy: 0.5992\n",
      "Epoch 25/50\n",
      "449/449 [==============================] - 12s 26ms/step - loss: 0.5331 - accuracy: 0.8087 - val_loss: 1.2227 - val_accuracy: 0.6052\n",
      "Epoch 26/50\n",
      "449/449 [==============================] - 13s 30ms/step - loss: 0.4866 - accuracy: 0.8288 - val_loss: 1.2339 - val_accuracy: 0.6042\n",
      "Epoch 27/50\n",
      "449/449 [==============================] - 14s 30ms/step - loss: 0.4506 - accuracy: 0.8405 - val_loss: 1.2853 - val_accuracy: 0.5970\n",
      "Epoch 28/50\n",
      "449/449 [==============================] - 14s 31ms/step - loss: 0.4148 - accuracy: 0.8544 - val_loss: 1.2950 - val_accuracy: 0.6006\n",
      "Epoch 29/50\n",
      "449/449 [==============================] - 15s 33ms/step - loss: 0.3854 - accuracy: 0.8639 - val_loss: 1.3524 - val_accuracy: 0.6000\n",
      "Epoch 30/50\n",
      "449/449 [==============================] - 15s 33ms/step - loss: 0.3603 - accuracy: 0.8721 - val_loss: 1.3731 - val_accuracy: 0.6023\n",
      "Epoch 31/50\n",
      "449/449 [==============================] - 15s 33ms/step - loss: 0.3346 - accuracy: 0.8830 - val_loss: 1.3990 - val_accuracy: 0.6011\n",
      "Epoch 32/50\n",
      "449/449 [==============================] - 15s 33ms/step - loss: 0.3133 - accuracy: 0.8911 - val_loss: 1.4334 - val_accuracy: 0.6003\n",
      "Epoch 33/50\n",
      "449/449 [==============================] - 15s 33ms/step - loss: 0.2876 - accuracy: 0.8997 - val_loss: 1.4272 - val_accuracy: 0.5992\n",
      "Epoch 34/50\n",
      "449/449 [==============================] - 15s 33ms/step - loss: 0.2669 - accuracy: 0.9088 - val_loss: 1.4788 - val_accuracy: 0.5992\n",
      "Epoch 35/50\n",
      "449/449 [==============================] - 15s 34ms/step - loss: 0.2490 - accuracy: 0.9136 - val_loss: 1.5194 - val_accuracy: 0.5995\n",
      "Epoch 36/50\n",
      "449/449 [==============================] - 15s 32ms/step - loss: 0.2357 - accuracy: 0.9183 - val_loss: 1.5385 - val_accuracy: 0.5933\n",
      "Epoch 37/50\n",
      "449/449 [==============================] - 14s 32ms/step - loss: 0.2230 - accuracy: 0.9247 - val_loss: 1.5322 - val_accuracy: 0.6020\n",
      "Epoch 38/50\n",
      "449/449 [==============================] - 15s 33ms/step - loss: 0.2074 - accuracy: 0.9291 - val_loss: 1.5970 - val_accuracy: 0.6016\n",
      "Epoch 39/50\n",
      "449/449 [==============================] - 14s 30ms/step - loss: 0.1980 - accuracy: 0.9334 - val_loss: 1.6046 - val_accuracy: 0.5982\n",
      "Epoch 40/50\n",
      "449/449 [==============================] - 13s 29ms/step - loss: 0.1838 - accuracy: 0.9361 - val_loss: 1.6436 - val_accuracy: 0.5967\n",
      "Epoch 41/50\n",
      "449/449 [==============================] - 13s 28ms/step - loss: 0.1767 - accuracy: 0.9401 - val_loss: 1.6990 - val_accuracy: 0.6042\n",
      "Epoch 42/50\n",
      "449/449 [==============================] - 13s 28ms/step - loss: 0.1657 - accuracy: 0.9442 - val_loss: 1.7070 - val_accuracy: 0.6020\n",
      "Epoch 43/50\n",
      "449/449 [==============================] - 12s 28ms/step - loss: 0.1620 - accuracy: 0.9460 - val_loss: 1.7213 - val_accuracy: 0.6074\n",
      "Epoch 44/50\n",
      "449/449 [==============================] - 12s 27ms/step - loss: 0.1494 - accuracy: 0.9510 - val_loss: 1.7298 - val_accuracy: 0.5999\n",
      "Epoch 45/50\n",
      "449/449 [==============================] - 14s 30ms/step - loss: 0.1417 - accuracy: 0.9525 - val_loss: 1.7817 - val_accuracy: 0.6007\n",
      "Epoch 46/50\n",
      "449/449 [==============================] - 15s 33ms/step - loss: 0.1411 - accuracy: 0.9532 - val_loss: 1.8005 - val_accuracy: 0.6020\n",
      "Epoch 47/50\n",
      "449/449 [==============================] - 13s 30ms/step - loss: 0.1381 - accuracy: 0.9529 - val_loss: 1.8693 - val_accuracy: 0.6063\n",
      "Epoch 48/50\n",
      "449/449 [==============================] - 14s 32ms/step - loss: 0.1349 - accuracy: 0.9562 - val_loss: 1.7960 - val_accuracy: 0.6023\n",
      "Epoch 49/50\n",
      "449/449 [==============================] - 15s 32ms/step - loss: 0.1255 - accuracy: 0.9589 - val_loss: 1.8042 - val_accuracy: 0.6028\n",
      "Epoch 50/50\n",
      "449/449 [==============================] - 14s 32ms/step - loss: 0.1190 - accuracy: 0.9617 - val_loss: 1.8210 - val_accuracy: 0.6045\n"
     ]
    }
   ],
   "source": [
    "emotion_model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.0001, decay=1e-6),metrics=['accuracy'])\n",
    "\n",
    "emotion_model_info = emotion_model.fit(\n",
    "        train_generator,\n",
    "        epochs=50,\n",
    "        batch_size=64,\n",
    "        validation_data=validation_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model.save('models/model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "099fd5c3e43525b17c255e8ba7b9c5b403e2029f8af62094b64bbe5ae73026c4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
